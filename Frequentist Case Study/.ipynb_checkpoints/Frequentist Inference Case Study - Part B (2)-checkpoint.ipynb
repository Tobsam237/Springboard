{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist Inference Case Study - Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Part B of the Frequentist inference case study! The purpose of this case study is to help you apply the concepts associated with Frequentist inference in Python. In particular, you'll practice writing Python code to apply the following statistical concepts: \n",
    "* the _z_-statistic\n",
    "* the _t_-statistic\n",
    "* the difference and relationship between the two\n",
    "* the Central Limit Theorem, including its assumptions and consequences\n",
    "* how to estimate the population mean and standard deviation from a sample\n",
    "* the concept of a sampling distribution of a test statistic, particularly for the mean\n",
    "* how to combine these concepts to calculate a confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used only data from a known normal distribution. **You'll now tackle real data, rather than simulated data, and answer some relevant real-world business problems using the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital medical charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a hospital has hired you as their data scientist. An administrator is working on the hospital's business operations plan and needs you to help them answer some business questions. \n",
    "\n",
    "In this assignment notebook, you're going to use frequentist statistical inference on a data sample to answer the questions:\n",
    "* has the hospital's revenue stream fallen below a key threshold?\n",
    "* are patients with insurance really charged different amounts than those without?\n",
    "\n",
    "Answering that last question with a frequentist approach makes some assumptions, and requires some knowledge, about the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some data on medical charges obtained from [Kaggle](https://www.kaggle.com/easonlai/sample-insurance-claim-prediction-dataset). \n",
    "\n",
    "For the purposes of this exercise, assume the observations are the result of random sampling from our single hospital. Recall that in the previous assignment, we introduced the Central Limit Theorem (CLT), and its consequence that the distributions of sample statistics approach a normal distribution as $n$ increases. The amazing thing about this is that it applies to the sampling distributions of statistics that have been calculated from even highly non-normal distributions of data! Recall, also, that hypothesis testing is very much based on making inferences about such sample statistics. You're going to rely heavily on the CLT to apply frequentist (parametric) tests to answer the questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "from numpy.random import seed\n",
    "medical = pd.read_csv(r\"C:\\Users\\Samuel Oluwatoba\\Downloads\\1585247986_Frequentist_Case_Study\\Frequentist Case Study\\insurance2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>insuranceclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
       "0   19    0  27.900         0       1       3  16884.92400               1\n",
       "1   18    1  33.770         1       0       2   1725.55230               1\n",
       "2   28    1  33.000         3       0       2   4449.46200               0\n",
       "3   33    1  22.705         0       0       1  21984.47061               0\n",
       "4   32    1  28.880         0       0       1   3866.85520               1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1:__ Plot the histogram of charges and calculate the mean and standard deviation. Comment on the appropriateness of these statistics for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(medical.charges, bins= 30)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 13270.422265141257   std: 12105.484975561605\n"
     ]
    }
   ],
   "source": [
    "sample_mean = np.mean(medical.charges)\n",
    "sample_std = np.std(medical.charges)\n",
    "print(\"mean:\", sample_mean, ' ',  'std:', sample_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2:__ The administrator is concerned that the actual average charge has fallen below 12,000, threatening the hospital's operational model. On the assumption that these data represent a random sample of charges, how would you justify that these data allow you to answer that question? And what would be the most appropriate frequentist test, of the ones discussed so far, to apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ The data is right skewed and is clearly not normally distributed. However, we can correctly justify our usage of t-statistic and z-statistic because the data sufficiently meets the asusmptions for CLT (Central Limit Theorem)\n",
    "\n",
    "1. The question already states that the data represents a random sample of charges. This satisfies the 'Randomization Condition'.\n",
    "2. Individual medical charges are independent of each other since they were randomly sampled. Also in the practical world, individual medical charges are independent events.\n",
    "3. Sample size is over 1300. The sample is not normally distributed but the big sample size makes up for the non-normality. Thus 'Sample Size Assumption' is met.\n",
    "\n",
    "From the CLT a good rule of thumb: sample distributions will usually be approximately normal if their sample size is n = 30 or larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q3:__ Given the nature of the administrator's concern, what is the appropriate confidence interval in this case? A ***one-sided*** or ***two-sided*** interval? (Refresh your understanding of this concept on p. 399 of the *AoS*). Calculate the critical value and the relevant 95% confidence interval for the mean, and comment on whether the administrator should be concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ One sided interval would be appropraite given the administrator's concern. \n",
    "\n",
    "The Null Hypothesis $H_0$ would be : There sample mean is 12000.\n",
    "\n",
    "The Alternative $H_1$ would be: sample mean is greater than 1200.\n",
    "\n",
    "* Moreover we would be calculating the left tail critical value since we want to know if the average charge has fallen below $12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The critical t value for 95% confidence interval is: -1.6459941145571324 \n"
     ]
    }
   ],
   "source": [
    "# calculating for Critical value\n",
    "# We're testing a value (12000) below the mean, thus p would be 0.05\n",
    "p = 0.05\n",
    "sample_size = medical.shape[0]\n",
    "df = sample_size - 1  # degrees of freedom \n",
    "\n",
    "t_critical = t.ppf(p, df)\n",
    "print(\"The critical t value for 95% confidence interval is: {} \".format(t_critical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The margin of error is: -544.7314053390936 \n"
     ]
    }
   ],
   "source": [
    "# Margin of Error:\n",
    "# moe = critical_value * standard_error\n",
    "\n",
    "standard_error = sample_std / (np.sqrt(sample_size))\n",
    "margin = t_critical *  standard_error\n",
    "print(\"The margin of error is: {} \".format(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus we can be confident 95% of the time that the true population mean lies above the value of 12725.69\n",
      "In conclusion, we reject the null hypothesis\n"
     ]
    }
   ],
   "source": [
    "interval_lower_bound = sample_mean + margin \n",
    "interval_lower_bound\n",
    "print('Thus we can be confident 95% of the time that the true population mean lies above the value of 12725.69')\n",
    "print('In conclusion, we reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The administrator then wants to know whether people with insurance really are charged a different amount to those without.\n",
    "\n",
    "__Q4:__ State the null and alternative hypothesis here. Use the _t_-test for the difference between means, where the pooled standard deviation of the two groups is given by:\n",
    "\\begin{equation}\n",
    "s_p = \\sqrt{\\frac{(n_0 - 1)s^2_0 + (n_1 - 1)s^2_1}{n_0 + n_1 - 2}}\n",
    "\\end{equation}\n",
    "\n",
    "and the *t*-test statistic is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\bar{x}_0 - \\bar{x}_1}{s_p \\sqrt{1/n_0 + 1/n_1}}.\n",
    "\\end{equation}\n",
    "\n",
    "(If you need some reminding of the general definition of ***t-statistic***, check out the definition on p. 404 of *AoS*). \n",
    "\n",
    "What assumption about the variances of the two groups are we making here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ Our assumption is that the two groups have the same variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q5:__ Perform this hypothesis test both manually, using the above formulae, and then using the appropriate function from [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-tests) (hint, you're looking for a function to perform a _t_-test on two independent samples). For the manual approach, calculate the value of the test statistic and then its probability (the p-value). Verify you get the same results from both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual Appraoch\n",
    "\n",
    "# Let's define insured and uninsured. From the data, Insuranceclaim: yes=1, no=0.\n",
    "insured = medical.charges[medical.insuranceclaim == 1]  \n",
    "uninsured = medical.charges[medical.insuranceclaim == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, find mean and std of insured and uninsured\n",
    "mean_insured = np.mean(insured)\n",
    "std_insured= np.std(insured)\n",
    "\n",
    "mean_uninsured = np.mean(uninsured)\n",
    "std_uninsured= np.std(uninsured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highlights parameters for pooled std and t-statistic to  solve manually\n",
    "\n",
    "n0 = len(insured)\n",
    "n1 = len(uninsured)\n",
    "s0 = np.std(insured)\n",
    "s1 = np.std(uninsured)\n",
    "x_bar0 = mean_insured\n",
    "x_bar1 = mean_uninsured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = np.sqrt( ((n0-1)*(s0)**2 + (n1-1)*(s1)**2)/ (n0+n1-2) )\n",
    "t = (x_bar1 - x_bar0)/(sp * np.sqrt(1/n0 + 1/n1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pooled standard deviation is: 11512.282899205744 \n",
      "The t critical value is: -11.901306943555385 \n"
     ]
    }
   ],
   "source": [
    "print(\"The pooled standard deviation is: {} \".format(sp))\n",
    "print(\"The t critical value is: {} \".format(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.893299030876712\n"
     ]
    }
   ],
   "source": [
    "#Now lets do the same using scipy.stats library\n",
    "from scipy.stats import ttest_ind\n",
    "#print(ttest_ind.__doc__)\n",
    "t_stats, p_value = ttest_ind(uninsured, insured, equal_var = True)\n",
    "print(t_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scipy's ttest_ind method we got approximately the same as when we solved manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Hopefully you got the exact same numerical results. This shows that you correctly calculated the numbers by hand. Secondly, you used the correct function and saw that it's much easier to use. All you need to do is pass your data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q6:__ Conceptual question: look through the documentation for statistical test functions in scipy.stats. You'll see the above _t_-test for a sample, but can you see an equivalent one for performing a *z*-test from a sample? Comment on your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".. _statsrefmanual:\n",
      "\n",
      "==========================================\n",
      "Statistical functions (:mod:`scipy.stats`)\n",
      "==========================================\n",
      "\n",
      ".. currentmodule:: scipy.stats\n",
      "\n",
      "This module contains a large number of probability distributions as\n",
      "well as a growing library of statistical functions.\n",
      "\n",
      "Each univariate distribution is an instance of a subclass of `rv_continuous`\n",
      "(`rv_discrete` for discrete distributions):\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   rv_continuous\n",
      "   rv_discrete\n",
      "   rv_histogram\n",
      "\n",
      "Continuous distributions\n",
      "========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   alpha             -- Alpha\n",
      "   anglit            -- Anglit\n",
      "   arcsine           -- Arcsine\n",
      "   argus             -- Argus\n",
      "   beta              -- Beta\n",
      "   betaprime         -- Beta Prime\n",
      "   bradford          -- Bradford\n",
      "   burr              -- Burr (Type III)\n",
      "   burr12            -- Burr (Type XII)\n",
      "   cauchy            -- Cauchy\n",
      "   chi               -- Chi\n",
      "   chi2              -- Chi-squared\n",
      "   cosine            -- Cosine\n",
      "   crystalball       -- Crystalball\n",
      "   dgamma            -- Double Gamma\n",
      "   dweibull          -- Double Weibull\n",
      "   erlang            -- Erlang\n",
      "   expon             -- Exponential\n",
      "   exponnorm         -- Exponentially Modified Normal\n",
      "   exponweib         -- Exponentiated Weibull\n",
      "   exponpow          -- Exponential Power\n",
      "   f                 -- F (Snecdor F)\n",
      "   fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n",
      "   fisk              -- Fisk\n",
      "   foldcauchy        -- Folded Cauchy\n",
      "   foldnorm          -- Folded Normal\n",
      "   frechet_r         -- Deprecated. Alias for weibull_min\n",
      "   frechet_l         -- Deprecated. Alias for weibull_max\n",
      "   genlogistic       -- Generalized Logistic\n",
      "   gennorm           -- Generalized normal\n",
      "   genpareto         -- Generalized Pareto\n",
      "   genexpon          -- Generalized Exponential\n",
      "   genextreme        -- Generalized Extreme Value\n",
      "   gausshyper        -- Gauss Hypergeometric\n",
      "   gamma             -- Gamma\n",
      "   gengamma          -- Generalized gamma\n",
      "   genhalflogistic   -- Generalized Half Logistic\n",
      "   geninvgauss       -- Generalized Inverse Gaussian\n",
      "   gilbrat           -- Gilbrat\n",
      "   gompertz          -- Gompertz (Truncated Gumbel)\n",
      "   gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n",
      "   gumbel_l          -- Left Sided Gumbel, etc.\n",
      "   halfcauchy        -- Half Cauchy\n",
      "   halflogistic      -- Half Logistic\n",
      "   halfnorm          -- Half Normal\n",
      "   halfgennorm       -- Generalized Half Normal\n",
      "   hypsecant         -- Hyperbolic Secant\n",
      "   invgamma          -- Inverse Gamma\n",
      "   invgauss          -- Inverse Gaussian\n",
      "   invweibull        -- Inverse Weibull\n",
      "   johnsonsb         -- Johnson SB\n",
      "   johnsonsu         -- Johnson SU\n",
      "   kappa4            -- Kappa 4 parameter\n",
      "   kappa3            -- Kappa 3 parameter\n",
      "   ksone             -- Kolmogorov-Smirnov one-sided (no stats)\n",
      "   kstwobign         -- Kolmogorov-Smirnov two-sided test for Large N (no stats)\n",
      "   laplace           -- Laplace\n",
      "   levy              -- Levy\n",
      "   levy_l\n",
      "   levy_stable\n",
      "   logistic          -- Logistic\n",
      "   loggamma          -- Log-Gamma\n",
      "   loglaplace        -- Log-Laplace (Log Double Exponential)\n",
      "   lognorm           -- Log-Normal\n",
      "   loguniform        -- Log-Uniform\n",
      "   lomax             -- Lomax (Pareto of the second kind)\n",
      "   maxwell           -- Maxwell\n",
      "   mielke            -- Mielke's Beta-Kappa\n",
      "   moyal             -- Moyal\n",
      "   nakagami          -- Nakagami\n",
      "   ncx2              -- Non-central chi-squared\n",
      "   ncf               -- Non-central F\n",
      "   nct               -- Non-central Student's T\n",
      "   norm              -- Normal (Gaussian)\n",
      "   norminvgauss      -- Normal Inverse Gaussian\n",
      "   pareto            -- Pareto\n",
      "   pearson3          -- Pearson type III\n",
      "   powerlaw          -- Power-function\n",
      "   powerlognorm      -- Power log normal\n",
      "   powernorm         -- Power normal\n",
      "   rdist             -- R-distribution\n",
      "   rayleigh          -- Rayleigh\n",
      "   rice              -- Rice\n",
      "   recipinvgauss     -- Reciprocal Inverse Gaussian\n",
      "   semicircular      -- Semicircular\n",
      "   skewnorm          -- Skew normal\n",
      "   t                 -- Student's T\n",
      "   trapz             -- Trapezoidal\n",
      "   triang            -- Triangular\n",
      "   truncexpon        -- Truncated Exponential\n",
      "   truncnorm         -- Truncated Normal\n",
      "   tukeylambda       -- Tukey-Lambda\n",
      "   uniform           -- Uniform\n",
      "   vonmises          -- Von-Mises (Circular)\n",
      "   vonmises_line     -- Von-Mises (Line)\n",
      "   wald              -- Wald\n",
      "   weibull_min       -- Minimum Weibull (see Frechet)\n",
      "   weibull_max       -- Maximum Weibull (see Frechet)\n",
      "   wrapcauchy        -- Wrapped Cauchy\n",
      "\n",
      "Multivariate distributions\n",
      "==========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   multivariate_normal   -- Multivariate normal distribution\n",
      "   matrix_normal         -- Matrix normal distribution\n",
      "   dirichlet             -- Dirichlet\n",
      "   wishart               -- Wishart\n",
      "   invwishart            -- Inverse Wishart\n",
      "   multinomial           -- Multinomial distribution\n",
      "   special_ortho_group   -- SO(N) group\n",
      "   ortho_group           -- O(N) group\n",
      "   unitary_group         -- U(N) group\n",
      "   random_correlation    -- random correlation matrices\n",
      "\n",
      "Discrete distributions\n",
      "======================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   bernoulli         -- Bernoulli\n",
      "   betabinom         -- Beta-Binomial\n",
      "   binom             -- Binomial\n",
      "   boltzmann         -- Boltzmann (Truncated Discrete Exponential)\n",
      "   dlaplace          -- Discrete Laplacian\n",
      "   geom              -- Geometric\n",
      "   hypergeom         -- Hypergeometric\n",
      "   logser            -- Logarithmic (Log-Series, Series)\n",
      "   nbinom            -- Negative Binomial\n",
      "   planck            -- Planck (Discrete Exponential)\n",
      "   poisson           -- Poisson\n",
      "   randint           -- Discrete Uniform\n",
      "   skellam           -- Skellam\n",
      "   zipf              -- Zipf\n",
      "   yulesimon         -- Yule-Simon\n",
      "\n",
      "An overview of statistical functions is given below.\n",
      "Several of these functions have a similar version in\n",
      "`scipy.stats.mstats` which work for masked arrays.\n",
      "\n",
      "Summary statistics\n",
      "==================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   describe          -- Descriptive statistics\n",
      "   gmean             -- Geometric mean\n",
      "   hmean             -- Harmonic mean\n",
      "   kurtosis          -- Fisher or Pearson kurtosis\n",
      "   mode              -- Modal value\n",
      "   moment            -- Central moment\n",
      "   skew              -- Skewness\n",
      "   kstat             --\n",
      "   kstatvar          --\n",
      "   tmean             -- Truncated arithmetic mean\n",
      "   tvar              -- Truncated variance\n",
      "   tmin              --\n",
      "   tmax              --\n",
      "   tstd              --\n",
      "   tsem              --\n",
      "   variation         -- Coefficient of variation\n",
      "   find_repeats\n",
      "   trim_mean\n",
      "   gstd              -- Geometric Standard Deviation\n",
      "   iqr\n",
      "   sem\n",
      "   bayes_mvs\n",
      "   mvsdist\n",
      "   entropy\n",
      "   median_absolute_deviation\n",
      "\n",
      "Frequency statistics\n",
      "====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   cumfreq\n",
      "   itemfreq\n",
      "   percentileofscore\n",
      "   scoreatpercentile\n",
      "   relfreq\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   binned_statistic     -- Compute a binned statistic for a set of data.\n",
      "   binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n",
      "   binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n",
      "\n",
      "Correlation functions\n",
      "=====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   f_oneway\n",
      "   pearsonr\n",
      "   spearmanr\n",
      "   pointbiserialr\n",
      "   kendalltau\n",
      "   weightedtau\n",
      "   linregress\n",
      "   siegelslopes\n",
      "   theilslopes\n",
      "   multiscale_graphcorr\n",
      "\n",
      "Statistical tests\n",
      "=================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ttest_1samp\n",
      "   ttest_ind\n",
      "   ttest_ind_from_stats\n",
      "   ttest_rel\n",
      "   kstest\n",
      "   chisquare\n",
      "   power_divergence\n",
      "   ks_2samp\n",
      "   epps_singleton_2samp\n",
      "   mannwhitneyu\n",
      "   tiecorrect\n",
      "   rankdata\n",
      "   ranksums\n",
      "   wilcoxon\n",
      "   kruskal\n",
      "   friedmanchisquare\n",
      "   brunnermunzel\n",
      "   combine_pvalues\n",
      "   jarque_bera\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ansari\n",
      "   bartlett\n",
      "   levene\n",
      "   shapiro\n",
      "   anderson\n",
      "   anderson_ksamp\n",
      "   binom_test\n",
      "   fligner\n",
      "   median_test\n",
      "   mood\n",
      "   skewtest\n",
      "   kurtosistest\n",
      "   normaltest\n",
      "\n",
      "Transformations\n",
      "===============\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   boxcox\n",
      "   boxcox_normmax\n",
      "   boxcox_llf\n",
      "   yeojohnson\n",
      "   yeojohnson_normmax\n",
      "   yeojohnson_llf\n",
      "   obrientransform\n",
      "   sigmaclip\n",
      "   trimboth\n",
      "   trim1\n",
      "   zmap\n",
      "   zscore\n",
      "\n",
      "Statistical distances\n",
      "=====================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   wasserstein_distance\n",
      "   energy_distance\n",
      "\n",
      "Random variate generation\n",
      "=========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   rvs_ratio_uniforms\n",
      "\n",
      "Circular statistical functions\n",
      "==============================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   circmean\n",
      "   circvar\n",
      "   circstd\n",
      "\n",
      "Contingency table functions\n",
      "===========================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   chi2_contingency\n",
      "   contingency.expected_freq\n",
      "   contingency.margins\n",
      "   fisher_exact\n",
      "\n",
      "Plot-tests\n",
      "==========\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   ppcc_max\n",
      "   ppcc_plot\n",
      "   probplot\n",
      "   boxcox_normplot\n",
      "   yeojohnson_normplot\n",
      "\n",
      "\n",
      "Masked statistics functions\n",
      "===========================\n",
      "\n",
      ".. toctree::\n",
      "\n",
      "   stats.mstats\n",
      "\n",
      "\n",
      "Univariate and multivariate kernel density estimation\n",
      "=====================================================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   gaussian_kde\n",
      "\n",
      "Warnings used in :mod:`scipy.stats`\n",
      "===================================\n",
      "\n",
      ".. autosummary::\n",
      "   :toctree: generated/\n",
      "\n",
      "   PearsonRConstantInputWarning\n",
      "   PearsonRNearConstantInputWarning\n",
      "\n",
      "For many more stat related functions install the software R and the\n",
      "interface package rpy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "print(scipy.stats.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__  Yeah, to perform a ztest, we use: scipy.stats.zscore as in the doc above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed this project notebook, you now have good hands-on experience:\n",
    "* using the central limit theorem to help you apply frequentist techniques to answer questions that pertain to very non-normally distributed data from the real world\n",
    "* performing inference using such data to answer business questions\n",
    "* forming a hypothesis and framing the null and alternative hypotheses\n",
    "* testing this using a _t_-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
